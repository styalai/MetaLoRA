{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-11T14:22:19.313404Z","iopub.execute_input":"2024-10-11T14:22:19.313815Z","iopub.status.idle":"2024-10-11T14:22:20.490050Z","shell.execute_reply.started":"2024-10-11T14:22:19.313773Z","shell.execute_reply":"2024-10-11T14:22:20.488939Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\"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\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:22:20.492133Z","iopub.execute_input":"2024-10-11T14:22:20.492752Z","iopub.status.idle":"2024-10-11T14:22:21.279133Z","shell.execute_reply.started":"2024-10-11T14:22:20.492700Z","shell.execute_reply":"2024-10-11T14:22:21.277868Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:22:21.280432Z","iopub.execute_input":"2024-10-11T14:22:21.280866Z","iopub.status.idle":"2024-10-11T14:22:34.547050Z","shell.execute_reply.started":"2024-10-11T14:22:21.280827Z","shell.execute_reply":"2024-10-11T14:22:34.545492Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\nfrom tqdm.auto import tqdm\nfrom sentence_transformers import SentenceTransformer\nfrom huggingface_hub import PyTorchModelHubMixin\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom huggingface_hub import HfApi\nfrom huggingface_hub import hf_hub_download, snapshot_download\nfrom pathlib import Path\nimport tempfile\nimport json\nimport os\nimport shutil\nfrom safetensors.torch import load_model, save_model","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:22:34.549576Z","iopub.execute_input":"2024-10-11T14:22:34.549986Z","iopub.status.idle":"2024-10-11T14:22:55.617265Z","shell.execute_reply.started":"2024-10-11T14:22:34.549942Z","shell.execute_reply":"2024-10-11T14:22:55.616209Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n#model_id = \"google/gemma-2-2b-it\"\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:22:55.618369Z","iopub.execute_input":"2024-10-11T14:22:55.619031Z","iopub.status.idle":"2024-10-11T14:22:58.329741Z","shell.execute_reply.started":"2024-10-11T14:22:55.618994Z","shell.execute_reply":"2024-10-11T14:22:58.328500Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd99554f732d4052bb975c90482508f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"913587c6775e408f8f92cd97ef9fc841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93dd16ab78d142d98807f28a4fb21b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"644f120e7b264069977d0289e6f86929"}},"metadata":{}}]},{"cell_type":"code","source":"class MLPGenerator(nn.Module, PyTorchModelHubMixin):\n    def __init__(self, mlora_layers, base_size, embd_size):\n        super().__init__()\n        self.mlora_layers = mlora_layers\n        self.base_size = base_size\n        self.embd_size = embd_size\n        \n    def init_generator(self, model):\n        # create a NN for each layer\n        groups = []\n        self.generator_modules = {}\n        self.generator_modules[\"baselinear\"] = nn.Linear(self.embd_size, self.base_size)\n        \n        for layer_name, param in model.named_parameters():\n            if layer_name in self.mlora_layers: # Layer chosen for MetaLoRA\n                group = layer_name.split(\".\")[2]\n                layer_name = layer_name.replace(\".\", \"_\")\n                \n                if group not in groups:\n                    self.generator_modules[group+\"_basenn\"] = nn.Sequential(\n                        nn.Linear(self.base_size, self.base_size*2),\n                        nn.ReLU(),\n                        nn.Linear(self.base_size*2, self.base_size*2),\n                        nn.ReLU(),\n                        nn.Linear(self.base_size*2, self.base_size)\n                    )\n                    self.group_module = {}\n                    groups.append(group)\n                \n                A_size, B_size = param.shape\n                self.group_module[layer_name+\"_A\"] = nn.Linear(self.base_size, A_size)\n                self.group_module[layer_name+\"_B\"] = nn.Linear(self.base_size, B_size)\n                self.generator_modules[group+\"_layers\"] = nn.ModuleDict(self.group_module)\n        \n        self.generator = nn.ModuleDict(self.generator_modules)\n\nclass EmbdModel(nn.Module, PyTorchModelHubMixin):\n    def __init__(self):\n        super().__init__()\n        self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n\n    def encode(self, text):\n        return self.model.encode([text], show_progress_bar=False)\n        \n        \nclass MLoRAmodel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model_name = config[\"model_name\"]\n        self.mlora_layers = config[\"mlora_layers\"]\n        self.base_size = config[\"base_size\"]\n        self.embd_size = config[\"embd_size\"]\n\n        \n    def push_to_hub(self, repo, token=None, push_generator=True, push_embd_model=True, private=False):\n        # Utiliser un dossier temporaire pour sauvegarder les modèles\n        api = HfApi(token=token)\n        repo_id = api.create_repo(repo_id=repo, exist_ok=True, private=private).repo_id\n        \n        with tempfile.TemporaryDirectory() as tmp:\n            saved_path = Path(tmp) / repo\n            \n            # Pousser le générateur dans un sous-dossier\n            if push_generator:\n                generator_path = saved_path / \"generator\"\n                generator_path.mkdir(parents=True, exist_ok=True)\n                self.generator.save_pretrained(generator_path)\n                \n                api.upload_folder(\n                    folder_path=generator_path,\n                    repo_id=repo_id,\n                    path_in_repo=\"generator\",\n                )\n                print(f\"Generator pushed to {repo}/generator\")\n            \n            # Pousser l'encoder dans un autre sous-dossier\n            \n            if push_embd_model:\n                embd_model_path = saved_path / \"embd_model\"\n                embd_model_path.mkdir(parents=True, exist_ok=True)\n                self.embd_model.save_pretrained(embd_model_path)\n                \n                api.upload_folder(\n                    folder_path=embd_model_path,\n                    repo_id=repo_id,\n                    path_in_repo=\"embd_model\",\n                )\n                print(f\"EmbdModel pushed to {repo}/embd_model\")\n            \n            ### push config\n            config_path = saved_path / \"config.json\"\n            with open(config_path, \"w\") as config_file:\n                json.dump(self.config, config_file, indent=4)\n\n            api.upload_file(\n                path_or_fileobj=config_path,\n                repo_id=repo_id,\n                path_in_repo=\"config.json\",  # Push to the main folder\n            )\n            print(f\"Config pushed to {repo}/config.json\")\n\n    @classmethod\n    def from_pretrained(cls, repo, token=None, load_generator=True, load_embd_model=True):\n        \n        with tempfile.TemporaryDirectory() as tmp:\n            # load the repo\n            snapshot_download(repo_id=repo, local_dir=f\"{tmp}/repo/\", token=token)\n            # create the instance of MLoRAmodel\n            config = open(f\"{tmp}/repo/config.json\")\n            config = json.load(config)\n            mloramodel = cls(config)\n            mloramodel.load_model()\n            #shutil.copyfile(f\"{tmp}/repo/config.json\", f\"{tmp}/repo/generator/config.json\")\n            \n            if load_generator:\n                mloramodel.generator = MLPGenerator(\n                    mlora_layers=config[\"mlora_layers\"], \n                    base_size=config[\"base_size\"], \n                    embd_size=config[\"embd_size\"]\n                )\n                mloramodel.generator.init_generator(mloramodel.model)\n                load_model(mloramodel.generator, f\"{tmp}/repo/generator/model.safetensors\")\n            if load_embd_model:\n                mloramodel.embd_model = EmbdModel()\n                \n        return mloramodel\n\n    def load_model(self, token=None):\n        print(\"load model...\")\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, token=token)\n        # set requires grad to False\n        for layer_name, param in self.model.named_parameters():\n            param.requires_grad = False # define the require_grad to False\n        print(\"model loaded\")\n    \n    \n    def init(self, generator=True, embd_model=True):\n        if generator:\n            self.generator = MLPGenerator(self.mlora_layers, self.base_size, self.embd_size)\n            self.generator.init_generator(self.model)\n        if embd_model:\n            self.embd_model = EmbdModel()\n    \n    \n    def MLoRA(self, base):\n        # generate the matrix from base and apply it to the weights\n        tp = []\n        groups = []\n        self.mlora_model = copy.deepcopy(self.model).to(base.device)\n        \n        for layer_name, param in self.mlora_model.named_parameters():\n            \n            if layer_name in self.mlora_layers:\n                group = layer_name.split(\".\")[2]\n                layer_name_underscore = layer_name.replace(\".\", \"_\")\n                \n                if group not in groups:\n                    base_for_group = self.generator.generator[group+\"_basenn\"](base)\n                    groups.append(group)\n                    \n                A = self.generator.generator[group+\"_layers\"][layer_name_underscore+\"_A\"](base_for_group).transpose(0, 1)\n                B = self.generator.generator[group+\"_layers\"][layer_name_underscore+\"_B\"](base_for_group)\n                AB = torch.matmul(A, B)\n    \n                param.data = param.data + AB\n  \n    \n    def run_MLoRA(self, text, device):\n        # run sentences-piece\n        embd = torch.tensor(self.embd_model.encode(text)) # (1, self.embd_size)\n        base = self.generator.generator[\"baselinear\"](embd.to(device)) # (1, self.base_size)\n        # run MLoRA\n        self.MLoRA(base)\n\n    \n    def forward(self, x):\n        # forward model\n        out = self.mlora_model.forward(x).logits\n        return out\n    \n    def del_model(self):\n        try:\n            del self.model\n            del self.mlora_model\n        except:\n            print(\"mlora or model doesn't exist\")\n    \n    def generate(self, input_ids, text, device, stream=False, tokenizer=None, max_length=50, do_sample=False, temperature=1.0, eos_token_id=2):\n        \"\"\"\n        Generate text sequences from input IDs using greedy or sampling-based decoding.\n\n        Parameters:\n        - input_ids (torch.Tensor): The input token IDs to start the generation.\n        - max_length (int): The maximum length of the generated sequence.\n        - do_sample (bool): Whether to use sampling instead of greedy decoding.\n        - temperature (float): Used to modulate the next token probabilities if sampling.\n\n        Returns:\n        - output_sequences (torch.Tensor): The generated sequence of token IDs.\n        \"\"\"\n        # Set model in evaluation mode\n        self.run_MLoRA(text, device)\n        self.model.eval()\n\n        # Initialize the output sequence with the input ids\n        output_sequences = input_ids\n\n        # Loop until max_length is reached\n        for _ in range(max_length):\n            # Forward pass through the model\n            outputs = self.forward(output_sequences)\n\n            # Get the logits for the last token\n            logits = outputs[:, -1, :]  # (batch_size, vocab_size)\n\n            if do_sample:\n                # Apply temperature and sample the next token from distribution\n                logits = logits / temperature\n                probs = torch.softmax(logits, dim=-1)\n                next_token = torch.multinomial(probs, num_samples=1)\n            else:\n                # Greedy decoding (select the token with the highest probability)\n                next_token = torch.argmax(logits, dim=-1, keepdim=True)\n\n            if stream:\n                print(tokenizer.decode(next_token[0]), end=\"\")\n            \n            # Append the next token to the output sequence\n            output_sequences = torch.cat([output_sequences, next_token], dim=-1)\n\n            # Stop generation if the next token is the end-of-sequence token (optional)\n            if next_token.item() == eos_token_id:\n                break\n        del self.mlora_model\n        return output_sequences","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:39:11.296157Z","iopub.execute_input":"2024-10-11T14:39:11.296625Z","iopub.status.idle":"2024-10-11T14:39:11.339948Z","shell.execute_reply.started":"2024-10-11T14:39:11.296579Z","shell.execute_reply":"2024-10-11T14:39:11.338590Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"config = {\n    \"model_name\": \"TinyLlama/TinyLlama-1.1B-Chat-v0.6\",\n    \"base_size\": 384,\n    \"embd_size\": 384,\n    \"mlora_layers\": [\n        \"model.layers.0.self_attn.q_proj.weight\",\n        \"model.layers.0.self_attn.k_proj.weight\",\n        \"model.layers.0.self_attn.v_proj.weight\",\n        \"model.layers.0.self_attn.o_proj.weight\",\n        \"model.layers.0.mlp.gate_proj.weight\",\n        \"model.layers.0.mlp.up_proj.weight\",\n        \"model.layers.0.mlp.down_proj.weight\",\n    ],\n}\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:39:12.180792Z","iopub.execute_input":"2024-10-11T14:39:12.181216Z","iopub.status.idle":"2024-10-11T14:39:12.187215Z","shell.execute_reply.started":"2024-10-11T14:39:12.181177Z","shell.execute_reply":"2024-10-11T14:39:12.185955Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"## load from hub\nmloramodel = MLoRAmodel.from_pretrained(\"Arthur-LAGACHERIE/testmlora\", token=None, load_generator=True, load_embd_model=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:39:16.237378Z","iopub.execute_input":"2024-10-11T14:39:16.237824Z","iopub.status.idle":"2024-10-11T14:39:24.562137Z","shell.execute_reply.started":"2024-10-11T14:39:16.237780Z","shell.execute_reply":"2024-10-11T14:39:24.560946Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb82fd1bb64548f2821241f955b36be3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/60.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7be7fc5727842faa360fff28a61d0fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generator/README.md:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7840346708e54ba8affc8e3012bb0f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/469 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a1d80db51d46feabfbfd14731454d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a2aaf54ef9c456396024ce7be38661e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generator/config.json:   0%|          | 0.00/377 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e333334bceb74344a118cdfc8ba37c20"}},"metadata":{}},{"name":"stdout","text":"load model...\nmodel loaded\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"## init from scratch\nmloramodel = MLoRAmodel(config)\nmloramodel.load_model(token=None)\nmloramodel.init(generator=True, embd_model=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:25:20.082141Z","iopub.execute_input":"2024-10-11T14:25:20.083110Z","iopub.status.idle":"2024-10-11T14:25:24.698556Z","shell.execute_reply.started":"2024-10-11T14:25:20.083066Z","shell.execute_reply":"2024-10-11T14:25:24.697502Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"load model...\nmodel loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"## push to hub\nmloramodel.push_to_hub(\"Arthur-LAGACHERIE/testmlora\", token=None, push_generator=True, push_embd_model=False, private=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:26:04.814608Z","iopub.execute_input":"2024-10-11T14:26:04.815041Z","iopub.status.idle":"2024-10-11T14:26:06.998010Z","shell.execute_reply.started":"2024-10-11T14:26:04.815001Z","shell.execute_reply":"2024-10-11T14:26:06.996648Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"Generator pushed to Arthur-LAGACHERIE/testmlora/generator\n","output_type":"stream"},{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"},{"name":"stdout","text":"Config pushed to Arthur-LAGACHERIE/testmlora/config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"hello how are you?\"\nx = tokenizer(text, return_tensors=\"pt\")['input_ids'].to(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:39:28.789881Z","iopub.execute_input":"2024-10-11T14:39:28.790354Z","iopub.status.idle":"2024-10-11T14:39:28.796291Z","shell.execute_reply.started":"2024-10-11T14:39:28.790309Z","shell.execute_reply":"2024-10-11T14:39:28.795291Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"out = mloramodel.generate(x, text, \"cpu\", stream=True, tokenizer=tokenizer, max_length=30)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T14:39:29.974063Z","iopub.execute_input":"2024-10-11T14:39:29.974473Z","iopub.status.idle":"2024-10-11T14:39:54.634123Z","shell.execute_reply.started":"2024-10-11T14:39:29.974437Z","shell.execute_reply":"2024-10-11T14:39:54.632973Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"\n\nJASON:\n(smiling)\nI'mdoinggreat,thanksforasking.\n\nCUTTO:\n\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.decode(out[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}